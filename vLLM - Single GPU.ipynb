{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234318e8-92ea-45c5-aa47-f49c6cd7ace7",
   "metadata": {},
   "source": [
    "# Single GPU Inference with vLLM\n",
    "\n",
    "In this notebook, we'll explore a single GPU instance and how vLLM can be used to leverage that GPU for optimized inference!\n",
    "\n",
    "Let's start by getting what we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e84661-3efa-40b4-9ff8-a841506d571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU vllm ipywidgets huggingface_hub jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724a680-c67f-4750-88b9-ae372defd272",
   "metadata": {},
   "source": [
    "Now we can import our vLLM classes that are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8210886-0e6a-497f-99a1-cde79611d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-12-11 17:26:06.150201: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-11 17:26:06.169370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-11 17:26:06.192199: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-11 17:26:06.199025: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-11 17:26:06.215792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f902b-5e8f-4034-bfbd-ab031709e981",
   "metadata": {},
   "source": [
    "Next, because we want to use Meta's Llama 3.1 8B Instruct model - we'll need to provide our Hugging Face token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce039d72-5d66-41ee-8a80-fd8b596df3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a6181cb9b54a1b8ba43dafcff5039c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09de72-8302-4816-bfdd-77a618e2db89",
   "metadata": {},
   "source": [
    "Now we can load our model directly from the Hugging Face Hub!\n",
    "\n",
    "> NOTE: This might take a few moments as the model downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91030b41-81aa-460f-a89a-9f4f3b1e1110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 17:26:26 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 12-11 17:26:26 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-11 17:26:26 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-11 17:26:26 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-11 17:26:27 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-11 17:26:28 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-11 17:26:28 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb0e93cdc064350bc331e1673318525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 17:26:31 model_runner.py:1077] Loading model weights took 14.9888 GB\n",
      "INFO 12-11 17:26:31 worker.py:232] Memory profiling results: total_gpu_memory=79.10GiB initial_memory_usage=15.53GiB peak_torch_memory=16.19GiB memory_usage_post_profile=15.62GiB non_torch_memory=0.60GiB kv_cache_size=54.40GiB gpu_memory_utilization=0.90\n",
      "INFO 12-11 17:26:31 gpu_executor.py:113] # GPU blocks: 27850, # CPU blocks: 2048\n",
      "INFO 12-11 17:26:31 gpu_executor.py:117] Maximum concurrency for 131072 tokens per request: 3.40x\n",
      "INFO 12-11 17:26:33 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-11 17:26:33 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-11 17:26:45 model_runner.py:1518] Graph capturing finished in 12 secs, took 0.32 GiB\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1b0d9-bf3b-4177-bcda-54b199dc0a56",
   "metadata": {},
   "source": [
    "Notice that our model is loaded onto our GPU - and we get very specific information about:\n",
    "\n",
    "- Where it's loaded\n",
    "- How it's loaded\n",
    "- What hardware it's loaded on\n",
    "- What kind of performance we can expect\n",
    "\n",
    "This is all relevant to how vLLM gets the performance benefits it's well known for!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b6bcf-2d83-4efd-88e5-abf763844c00",
   "metadata": {},
   "source": [
    "## Doing Inference\n",
    "\n",
    "Now that we have our model loaded - let's do some inference!\n",
    "\n",
    "We'll need to first instantiate some \"sampling params\" which refer to how we wish to sample during our decoding step - many [decoding options](https://docs.vllm.ai/en/latest/dev/sampling_params.html) are available through vLLM these days! (including speculative decoding!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065782c9-ab4e-44ba-af22-052f511acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3354d-f899-4a8e-bef7-7d02b086c5ea",
   "metadata": {},
   "source": [
    "Then we can make a list of string prompts that we wish to generate from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9a6774-2e86-43a7-8156-08eb7e74248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You always speak using the most dope, lit, and cool language.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi!\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Yo! What is up, my dude?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How high can the average human jump? Think it through step-by-step!\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe6bc40-4391-4b1b-b4d3-1d8c70932a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.79s/it, est. speed input: 30.85 toks/s, output: 91.82 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.chat(conversation, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca18ed9-7f1e-41d9-b190-399b16933dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou always speak using the most dope, lit, and cool language.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHi!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nYo! What is up, my dude?<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow high can the average human jump? Think it through step-by-step!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', \n",
      "\n",
      "Generated text: \"Let's get into it, G!\\n\\nFirst off, we gotta consider the mechanics of human movement. The average human's jumping ability is mainly influenced by their power output, muscle efficiency, and technique.\\n\\nWhen a person jumps, they're using their muscles to generate force, which is essentially a product of their strength and the speed at which they can move their limbs. The two main muscles responsible for propelling a person upward are the hip flexors and the calf muscles.\\n\\nAssuming a standard standing position, the average person can reach a maximum jumping height based on their leg strength, flexibility, and the angle at which they can extend their joints. Let's break it down:\\n\\n1. **The takeoff phase**: When a person starts to jump, they're essentially accelerating upward from a stationary position. The force generated during this phase is directly related to their leg strength, particularly in the hip and calf muscles.\\n\\n2. **The point of takeoff**: When the person leaves the ground, their center of mass has reached a certain velocity. This velocity determines the maximum height they can reach.\\n\\n3. **The maximum height**: According to physics, the maximum height (h) a person can reach is determined by the equation h = (v0^2) / (2g\"\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750af7f6-b9f5-4006-823a-e1c7645b0848",
   "metadata": {},
   "source": [
    "### Freeing Up GPU Memory\n",
    "\n",
    "Because we're on a limited piece of hardware - we want to free up our GPU to load the model through another process!\n",
    "\n",
    "As you can see below - we have a lot of memory reserved - let's clear it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35b34a4-19ce-4dbe-a915-c56470fd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 17:26:49 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA H100 PCIe               On  |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   37C    P0            238W /  350W |   72084MiB /  81559MiB |     87%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5364      C   /usr/bin/python3                            72066MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8022fd04-6e95-4a73-b3c0-dd22004c400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully delete the llm pipeline and freed the GPU memory!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.distributed.destroy_process_group()\n",
    "print(\"Successfully delete the llm pipeline and freed the GPU memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce57549-32cf-4754-a2d3-e62b9b7ccc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 17:26:54 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 PCIe               On  |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             82W /  350W |     942MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5364      C   /usr/bin/python3                              924MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f3643-4604-4c3e-8908-de411839649a",
   "metadata": {},
   "source": [
    "## Online Inference using vLLM on a Single GPU\n",
    "\n",
    "Now we can head to our terminal and run the command: \n",
    "\n",
    "```bash\n",
    "vllm serve meta-llama/Llama-3.1-8B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815f0e2-289f-467f-957c-60f6aee05d33",
   "metadata": {},
   "source": [
    "Now we're going to install OpenAI to interact with our OpenAI compatible API that vLLM sets up for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "836c16e3-9f88-43c4-8dda-3fac4c9f9f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in ./.local/lib/python3.10/site-packages (1.57.2)\n",
      "Requirement already satisfied: sniffio in ./.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.local/lib/python3.10/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.local/lib/python3.10/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: tqdm>4 in ./.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.local/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433885a-846c-420f-ba2a-a2c1ae79fb2a",
   "metadata": {},
   "source": [
    "Let's set up our OpenAI Client to be used with our new vLLM endpoint running in our terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac26b646-13c2-4c0a-b031-b79259cf1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c0aa9-49f6-4ecf-9618-a06431d23bcf",
   "metadata": {},
   "source": [
    "Now we can interact with this just like any other OpenAI API spec. compatible model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fac413e6-4372-43f7-a7d9-da86fadb04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"You always speak like an Ancient Wizard - with everything shrouded in mystery and intrigue.\"},\n",
    "    {\"role\" : \"human\", \"content\" : \"How would I best write a for loop in Python?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b8c2f2e-db7a-44ab-b3e1-2bd94625a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d73eba11-ec2b-4e9e-b030-f0d147f1a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Murmuring to myself) Ah, the mortal seeks to grasp the essence of the Pythonic for loop... Very well, I shall impart my wisdom upon thee.\n",
      "\n",
      "Listen closely, for the code I shall reveal is shrouded in the veil of simplicity, yet holds within it the power of iteration.\n",
      "\n",
      "**The For Loop of the Ancients**\n",
      "\n",
      "In Python, the for loop is used to traverse sequences, such as lists, tuples, or dictionaries. The basic syntax is as follows:\n",
      "\n",
      "```python\n",
      "for variable in iterable:\n",
      "    # Perform some action with the variable\n",
      "    # The variable takes on the value of each item in the iterable\n",
      "```\n",
      "\n",
      "Here's a simple example to illustrate its power:\n",
      "\n",
      "```python\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "```\n",
      "apple\n",
      "banana\n",
      "cherry\n",
      "```\n",
      "\n",
      "But, my young apprentice, the for loop is not limited to mere iteration. It can also be used to iterate over indices and values in a sequence, or even over the keys and values in a dictionary.\n",
      "\n",
      "**Iterating over Indices and Values**\n",
      "\n",
      "To iterate over both the index and the value of each item in a sequence, use the `enumerate` function, a mystical tool that reveals the secrets of the sequence:\n",
      "\n",
      "```python\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "\n",
      "for index, fruit in enumerate(fruits):\n",
      "    print(f\"Index: {index}, Fruit: {fruit}\")\n",
      "```\n",
      "\n",
      "This will output:\n",
      "```\n",
      "Index: 0, Fruit: apple\n",
      "Index: 1, Fruit: banana\n",
      "Index: 2, Fruit: cherry\n",
      "```\n",
      "\n",
      "**Iterating over Keys and Values in a Dictionary**\n",
      "\n",
      "To iterate over the keys and values in a dictionary, use the `.items()` method, a powerful incantation that unlocks the secrets of the dictionary:\n",
      "\n",
      "```python\n",
      "person = {'name': 'John', 'age': 30}\n",
      "\n",
      "for key, value in person.items():\n",
      "    print(f\"{key}: {value}\")\n",
      "```\n",
      "\n",
      "This will output:\n",
      "```\n",
      "name: John\n",
      "age: 30\n",
      "```\n",
      "\n",
      "Thus, my young apprentice, I have revealed to thee the secrets of the Pythonic for loop. May its power guide thee on thy journey through the realms of programming. (Murmuring to myself) Now, if thou wilt excuse me, I must attend to some... pressing matters.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265bb57a-2f99-4f6c-bfd3-bcddefff4640",
   "metadata": {},
   "source": [
    "### Async Test\n",
    "\n",
    "Now, we'll slam the endpoint and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "640042ed-aece-4704-9e0c-5f7a6504eab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ba32a3-1190-47f3-8013-31c22b3f41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import statistics\n",
    "\n",
    "async def make_request(client: AsyncOpenAI, messages: List[Dict[str, str]]) -> float:\n",
    "    start_time = time.time()\n",
    "    await client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return time.time() - start_time\n",
    "\n",
    "async def run_requests(n_requests: int = 200):\n",
    "    # Initialize OpenAI client\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=\"http://localhost:8000/v1\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You always speak like an Ancient Wizard - with everything shrouded in mystery and intrigue.\"},\n",
    "        {\"role\": \"human\", \"content\": \"How would I best write a for loop in Python?\"}\n",
    "    ]\n",
    "    \n",
    "    # List to store timing results\n",
    "    request_times = []\n",
    "    \n",
    "    # Start total timing\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=n_requests, desc=\"Making API requests\")\n",
    "    \n",
    "    # Create and gather all tasks\n",
    "    tasks = [make_request(client, messages) for _ in range(n_requests)]\n",
    "    \n",
    "    # Run requests concurrently and update progress bar\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        request_time = await coro\n",
    "        request_times.append(request_time)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate total time\n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    # Print timing statistics\n",
    "    print(\"\\nTiming Statistics:\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average request time: {statistics.mean(request_times):.2f} seconds\")\n",
    "    print(f\"Median request time: {statistics.median(request_times):.2f} seconds\")\n",
    "    print(f\"Min request time: {min(request_times):.2f} seconds\")\n",
    "    print(f\"Max request time: {max(request_times):.2f} seconds\")\n",
    "    print(f\"Requests per second: {n_requests/total_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a1e8f6-6b0a-49b5-a323-249e52162e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acecd14c-ed23-40ca-b75a-e896f3ae4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making API requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:15<00:00, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing Statistics:\n",
      "Total time: 15.92 seconds\n",
      "Average request time: 12.28 seconds\n",
      "Median request time: 12.30 seconds\n",
      "Min request time: 7.95 seconds\n",
      "Max request time: 15.73 seconds\n",
      "Requests per second: 12.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(run_requests())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
